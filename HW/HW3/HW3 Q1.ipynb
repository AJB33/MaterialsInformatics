{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Bayesian Optimization By Hand\n",
    "\n",
    "While there are a variety of libraries that can be used to perform Bayesian optimization, it is important to understand the underlying process. In this problem, you will build a bayesian optimization pipeline by hand and use it to find 15 of the top 5% of materials in a dataset.\n",
    "\n",
    "Taken as a whole, this problem may seem daunting. However, when broken into its components it is quite manageable. Recall the Bayesian optimization flow chart from the lectures that details to core components of the a bayesian optimization pipeline. Rather than trying to solve this problem all in one shot, we recommend building the pipeline one component at a time and making sure you understand it before moving on to the next component.\n",
    "\n",
    "You are given the crossed barrel dataset, which shows the toughness of materials as a function of several 3D printer parameters. For learning purposes, we are going to pretend that we don't know the optimal parameters in advance and perform a simulated optimzation campaign to find them. Your task is to start with 5 random samples from the data and use Bayesian optimization to find the top 5% of candidates in the data. Use the gaussain process model included in the `scikit-learn` package as your surrogate model. We would like you to perform the optimization with two different acquisition functions: expected improvement and upper confidence bound. You will need to implement these acquisition functions by hand (don't worry it's easy). Your termination criteria will be when you have found at least 15 candidates from the top 5% of the materials in the dataset.\n",
    "\n",
    "Your code will likely get very messy as you experiment and learn how to implement these components. However, clean code is an expectation in professional settings. Therefore, we are requiring that your final code by clean and commented (this includes function explanations) upon submission. 3pts will be assigned to this. If you are unsure if your code is clean enough, ask a TA or instructor for feedback.\n",
    "\n",
    "**Specific Tasks**\n",
    "\n",
    "Please show the following for both UCB and EI cases:\n",
    "- How many iterations it took to find 15 candidates (should be less than 150)\n",
    "- A plot of the 15 candidates you found relative to the entire dataset\n",
    "- A plot of the number of candidates you found as a function of iteation count\n",
    "- A plot of the best candidate you found as a function of the iteration count\n",
    "\n",
    "Finally provide some commentary on the performance of your optimization campaigns. What surprised you about the results? What would you do differently if you were to do this in a real experimental setting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.654425415833334\n",
      "[323, 511, 378, 68, 43]\n",
      "[array([ 0.4472136 , -1.15044748, -0.62622429,  1.22474487]), array([ 1.34164079, -0.38348249,  0.62622429,  0.        ]), array([ 0.4472136 ,  0.        ,  0.93933644, -1.22474487]), array([-1.34164079,  0.        , -1.56556073,  1.22474487]), array([-1.34164079, -0.76696499,  0.31311215,  0.        ])]\n",
      "[7.465714901666666, 34.958582676666666, 30.1531902, 1.6541604416666666, 3.5997365583333334]\n",
      "          n     theta         r         t       ucb\n",
      "0  0.447214 -1.150447 -0.626224  1.224745  1.445742\n",
      "1  1.341641 -0.383482  0.626224  0.000000  0.674370\n",
      "2  0.447214  0.000000  0.939336 -1.224745  0.286585\n",
      "3 -1.341641  0.000000 -1.565561  1.224745  2.033313\n",
      "4 -1.341641 -0.766965  0.313112  0.000000  0.633695\n"
     ]
    }
   ],
   "source": [
    "#your code goes here\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "# import data as a pandas dataframe\n",
    "data = pd.read_csv(\"data/crossed_barrel_dataset.csv\")\n",
    "\n",
    "# create a array of the average values of each of the toughness parameters\n",
    "# these will be the independent variables used to predict the target value\n",
    "averaged_data = data.groupby(['n','theta','r','t']).mean().reset_index()\n",
    "\n",
    "# the target value, or labels, of the independent variables will be toughness\n",
    "target = averaged_data.pop('toughness')\n",
    "\n",
    "# scale the averaged data array so that the relative magnitudes \n",
    "# of their values do not confuse the ML model\n",
    "scaler = StandardScaler()\n",
    "averaged_data_standardized = scaler.fit_transform(averaged_data)\n",
    "\n",
    "# define a target value within the first standard deviation of the data\n",
    "cutoff = target.quantile(0.5)\n",
    "\n",
    "# generate five random indices to be used \n",
    "indices = random.sample(range(0,600),5)\n",
    "\n",
    "# Use the randomly generated indices to select random data from the averaged data\n",
    "# dataset as the starting points for our Bayesian optimization\n",
    "starting_values = [averaged_data_standardized[i] for i in indices [:5]]\n",
    "\n",
    "# define the corresponding labels to the data that was randomly selected\n",
    "starting_values_target = [target[i] for i in indices [:5]]\n",
    "\n",
    "# Use the radial basis function from sklearn\n",
    "kernel = RBF()\n",
    "# define our model as a Gaussian Process Regressor\n",
    "surrogate_model = GaussianProcessRegressor(kernel=kernel)\n",
    "# fit the GPR to the starting values\n",
    "surrogate_model.fit(starting_values,starting_values_target)\n",
    "#\n",
    "exploration_param =1\n",
    "#\n",
    "mean_predictions, std_predictions = surrogate_model.predict(averaged_data_standardized, return_std=True)\n",
    "# upper confidence bound\n",
    "ucb = mean_predictions + exploration_param*std_predictions\n",
    "\n",
    "# append updated values to df\n",
    "df1 = pd.DataFrame(starting_values,columns=['n','theta','r','t'])\n",
    "df2 = pd.DataFrame(ucb,columns=['ucb'])\n",
    "df = df1.join(df2)\n",
    "\n",
    "# loop until stopping criterion is reached\n",
    "# define stopping criterion as when \n",
    "\n",
    "while \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper Confidence Bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MatInformatics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
